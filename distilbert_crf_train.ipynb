{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T01:31:58.283713Z",
     "start_time": "2024-03-11T01:31:57.452652Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load The Data\n",
    "import json\n",
    "import os\n",
    "#raw = json.load(open('./data/annotations_consistent.json'))\n",
    "\n",
    "raw = json.load(open('./data/FinEntity.json'))\n",
    "# raw = data.get(\"examples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T01:31:59.071533Z",
     "start_time": "2024-03-11T01:31:58.285729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n",
      "13\n",
      "987\n"
     ]
    }
   ],
   "source": [
    "## Preparing Sequence Labeling Data for Transformers\n",
    "from sequence_aligner.labelset import LabelSet\n",
    "from sequence_aligner.dataset import TrainingDatasetCRF\n",
    "from sequence_aligner.containers import TraingingBatch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis')\n",
    "label_set = LabelSet(labels=[\"Neutral\", \"Positive\", \"Negative\"])  # label in this dataset\n",
    "print(label_set.ids_to_label)\n",
    "print(len(label_set.ids_to_label.values()))\n",
    "dataset = TrainingDatasetCRF(data=raw, tokenizer=tokenizer, label_set=label_set,tokens_per_batch = 128)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4148, 5, 1313, 526, 6, 16422, 1290, 16, 18142, 231, 207, 71, 10, 8600, 11, 3472, 3365, 8, 9805, 4403, 8003, 6083, 424, 7387, 16, 67, 62, 30, 231, 207, 71, 1963, 1451, 2113, 4, 1437, 1437, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "On the positive side, Siemens is rallying 6% after a boom in quarterly orders and packaging maker Huhtamaki is also up by 6% after profit beat expectations.  <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n"
     ]
    }
   ],
   "source": [
    "## Prepare train data and valid data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import config\n",
    "train_size = int(config.dev_split_size * len(dataset))\n",
    "validate_size = len(dataset) - train_size\n",
    "train_dataset, validate_dataset = random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "val_loader = DataLoader(validate_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "\n",
    "print(dataset[1].input_ids)\n",
    "print(dataset[1].labels)\n",
    "print(dataset[1].attention_masks)\n",
    "print(tokenizer.decode(dataset[1].input_ids))\n",
    "print(dataset.label_set.ids_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertCrfForNer were not initialized from the model checkpoint at mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======START TRAIN EPOCHS 1=======\n",
      "Epoch: 1, train Loss:41.8305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       103\n",
      "     Neutral       0.00      0.00      0.00       244\n",
      "    Positive       0.00      0.00      0.00        93\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       440\n",
      "   macro avg       0.00      0.00      0.00       440\n",
      "weighted avg       0.00      0.00      0.00       440\n",
      "\n",
      "Epoch: 1, train Loss:22.6151\n",
      "=======START TRAIN EPOCHS 2=======\n",
      "Epoch: 2, train Loss:17.2639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.50      0.06      0.10       103\n",
      "     Neutral       0.54      0.09      0.15       244\n",
      "    Positive       0.51      0.20      0.29        93\n",
      "\n",
      "   micro avg       0.52      0.10      0.17       440\n",
      "   macro avg       0.52      0.12      0.18       440\n",
      "weighted avg       0.52      0.10      0.17       440\n",
      "\n",
      "Epoch: 2, train Loss:15.1513\n",
      "=======START TRAIN EPOCHS 3=======\n",
      "Epoch: 3, train Loss:11.0553\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.29      0.06      0.10       103\n",
      "     Neutral       0.53      0.19      0.28       244\n",
      "    Positive       0.48      0.31      0.38        93\n",
      "\n",
      "   micro avg       0.49      0.19      0.27       440\n",
      "   macro avg       0.43      0.19      0.25       440\n",
      "weighted avg       0.47      0.19      0.26       440\n",
      "\n",
      "Epoch: 3, train Loss:12.0735\n",
      "=======START TRAIN EPOCHS 4=======\n",
      "Epoch: 4, train Loss:7.5445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.40      0.08      0.13       103\n",
      "     Neutral       0.52      0.31      0.39       244\n",
      "    Positive       0.49      0.40      0.44        93\n",
      "\n",
      "   micro avg       0.50      0.27      0.35       440\n",
      "   macro avg       0.47      0.26      0.32       440\n",
      "weighted avg       0.49      0.27      0.34       440\n",
      "\n",
      "Epoch: 4, train Loss:10.8320\n",
      "=======START TRAIN EPOCHS 5=======\n",
      "Epoch: 5, train Loss:5.4398\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.30      0.17      0.21       103\n",
      "     Neutral       0.52      0.40      0.45       244\n",
      "    Positive       0.49      0.29      0.36        93\n",
      "\n",
      "   micro avg       0.47      0.32      0.38       440\n",
      "   macro avg       0.44      0.29      0.34       440\n",
      "weighted avg       0.46      0.32      0.38       440\n",
      "\n",
      "Epoch: 5, train Loss:9.5165\n",
      "=======START TRAIN EPOCHS 6=======\n",
      "Epoch: 6, train Loss:4.0339\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.37      0.17      0.24       103\n",
      "     Neutral       0.58      0.40      0.47       244\n",
      "    Positive       0.42      0.39      0.40        93\n",
      "\n",
      "   micro avg       0.50      0.34      0.41       440\n",
      "   macro avg       0.46      0.32      0.37       440\n",
      "weighted avg       0.50      0.34      0.40       440\n",
      "\n",
      "Epoch: 6, train Loss:8.8487\n",
      "=======START TRAIN EPOCHS 7=======\n",
      "Epoch: 7, train Loss:3.1608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.36      0.21      0.27       103\n",
      "     Neutral       0.49      0.45      0.47       244\n",
      "    Positive       0.44      0.35      0.39        93\n",
      "\n",
      "   micro avg       0.46      0.37      0.41       440\n",
      "   macro avg       0.43      0.34      0.38       440\n",
      "weighted avg       0.45      0.37      0.41       440\n",
      "\n",
      "Epoch: 7, train Loss:9.0069\n",
      "=======START TRAIN EPOCHS 8=======\n",
      "Epoch: 8, train Loss:2.4506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.39      0.31      0.34       103\n",
      "     Neutral       0.51      0.49      0.50       244\n",
      "    Positive       0.48      0.25      0.33        93\n",
      "\n",
      "   micro avg       0.48      0.40      0.43       440\n",
      "   macro avg       0.46      0.35      0.39       440\n",
      "weighted avg       0.47      0.40      0.43       440\n",
      "\n",
      "Epoch: 8, train Loss:8.5609\n",
      "=======START TRAIN EPOCHS 9=======\n",
      "Epoch: 9, train Loss:2.0234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.23      0.27       103\n",
      "     Neutral       0.63      0.38      0.47       244\n",
      "    Positive       0.38      0.39      0.39        93\n",
      "\n",
      "   micro avg       0.48      0.35      0.40       440\n",
      "   macro avg       0.45      0.33      0.38       440\n",
      "weighted avg       0.50      0.35      0.41       440\n",
      "\n",
      "Epoch: 9, train Loss:8.8861\n",
      "=======START TRAIN EPOCHS 10=======\n",
      "Epoch: 10, train Loss:1.6422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.23      0.27       103\n",
      "     Neutral       0.54      0.46      0.50       244\n",
      "    Positive       0.39      0.34      0.37        93\n",
      "\n",
      "   micro avg       0.46      0.38      0.42       440\n",
      "   macro avg       0.42      0.35      0.38       440\n",
      "weighted avg       0.46      0.38      0.42       440\n",
      "\n",
      "Epoch: 10, train Loss:8.6562\n",
      "=======START TRAIN EPOCHS 11=======\n",
      "Epoch: 11, train Loss:1.3429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.37      0.21      0.27       103\n",
      "     Neutral       0.52      0.48      0.50       244\n",
      "    Positive       0.40      0.38      0.39        93\n",
      "\n",
      "   micro avg       0.47      0.40      0.43       440\n",
      "   macro avg       0.43      0.36      0.39       440\n",
      "weighted avg       0.46      0.40      0.42       440\n",
      "\n",
      "Epoch: 11, train Loss:8.4116\n",
      "=======START TRAIN EPOCHS 12=======\n",
      "Epoch: 12, train Loss:1.1436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.25      0.28       103\n",
      "     Neutral       0.50      0.46      0.48       244\n",
      "    Positive       0.42      0.31      0.36        93\n",
      "\n",
      "   micro avg       0.45      0.38      0.41       440\n",
      "   macro avg       0.41      0.34      0.37       440\n",
      "weighted avg       0.44      0.38      0.41       440\n",
      "\n",
      "Epoch: 12, train Loss:8.8307\n",
      "=======START TRAIN EPOCHS 13=======\n",
      "Epoch: 13, train Loss:1.0315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.23      0.27       103\n",
      "     Neutral       0.54      0.47      0.50       244\n",
      "    Positive       0.38      0.38      0.38        93\n",
      "\n",
      "   micro avg       0.46      0.39      0.42       440\n",
      "   macro avg       0.41      0.36      0.38       440\n",
      "weighted avg       0.45      0.39      0.42       440\n",
      "\n",
      "Epoch: 13, train Loss:8.6756\n",
      "=======START TRAIN EPOCHS 14=======\n",
      "Epoch: 14, train Loss:0.9125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.27      0.30       103\n",
      "     Neutral       0.54      0.46      0.50       244\n",
      "    Positive       0.39      0.35      0.37        93\n",
      "\n",
      "   micro avg       0.46      0.39      0.42       440\n",
      "   macro avg       0.42      0.36      0.39       440\n",
      "weighted avg       0.46      0.39      0.42       440\n",
      "\n",
      "Epoch: 14, train Loss:8.3158\n",
      "=======START TRAIN EPOCHS 15=======\n",
      "Epoch: 15, train Loss:0.8505\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.32      0.24      0.27       103\n",
      "     Neutral       0.52      0.47      0.49       244\n",
      "    Positive       0.42      0.33      0.37        93\n",
      "\n",
      "   micro avg       0.46      0.39      0.42       440\n",
      "   macro avg       0.42      0.35      0.38       440\n",
      "weighted avg       0.45      0.39      0.42       440\n",
      "\n",
      "Epoch: 15, train Loss:8.7360\n",
      "=======START TRAIN EPOCHS 16=======\n",
      "Epoch: 16, train Loss:0.8025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.31      0.22      0.26       103\n",
      "     Neutral       0.54      0.47      0.50       244\n",
      "    Positive       0.43      0.38      0.40        93\n",
      "\n",
      "   micro avg       0.47      0.39      0.43       440\n",
      "   macro avg       0.43      0.36      0.39       440\n",
      "weighted avg       0.47      0.39      0.42       440\n",
      "\n",
      "Epoch: 16, train Loss:8.5400\n",
      "=======START TRAIN EPOCHS 17=======\n",
      "Epoch: 17, train Loss:0.7836\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.31      0.22      0.26       103\n",
      "     Neutral       0.54      0.47      0.50       244\n",
      "    Positive       0.43      0.38      0.40        93\n",
      "\n",
      "   micro avg       0.47      0.39      0.43       440\n",
      "   macro avg       0.43      0.36      0.39       440\n",
      "weighted avg       0.47      0.39      0.42       440\n",
      "\n",
      "Epoch: 17, train Loss:8.9383\n",
      "=======START TRAIN EPOCHS 18=======\n",
      "Epoch: 18, train Loss:0.7844\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.31      0.22      0.26       103\n",
      "     Neutral       0.54      0.47      0.50       244\n",
      "    Positive       0.43      0.38      0.40        93\n",
      "\n",
      "   micro avg       0.47      0.39      0.43       440\n",
      "   macro avg       0.43      0.36      0.39       440\n",
      "weighted avg       0.47      0.39      0.42       440\n",
      "\n",
      "Epoch: 18, train Loss:8.5506\n",
      "=======START TRAIN EPOCHS 19=======\n",
      "Epoch: 19, train Loss:0.8015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.31      0.22      0.26       103\n",
      "     Neutral       0.54      0.47      0.50       244\n",
      "    Positive       0.43      0.38      0.40        93\n",
      "\n",
      "   micro avg       0.47      0.39      0.43       440\n",
      "   macro avg       0.43      0.36      0.39       440\n",
      "weighted avg       0.47      0.39      0.42       440\n",
      "\n",
      "Epoch: 19, train Loss:8.7451\n",
      "=======START TRAIN EPOCHS 20=======\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=======START TRAIN EPOCHS \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m=======\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m(e\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 46\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     valid_epoch(e, model, val_loader,device,label_set)\n",
      "File \u001b[1;32mc:\\Users\\HP\\PycharmProjects\\PJ\\sentiments\\util\\train.py:55\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(e, model, data_loader, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m fgm\u001b[38;5;241m.\u001b[39mattack() \n\u001b[0;32m     54\u001b[0m loss_adv \u001b[38;5;241m=\u001b[39m model( \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 55\u001b[0m \u001b[43mloss_adv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     56\u001b[0m fgm\u001b[38;5;241m.\u001b[39mrestore() \n\u001b[0;32m     58\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## DistilBert+Crf\n",
    "import warnings\n",
    "from model.DistilBERT_crf import DistilBertCrfForNer\n",
    "from seqeval import metrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from seqeval.metrics import f1_score, precision_score, accuracy_score\n",
    "from torch import cuda\n",
    "import config\n",
    "from util.train import train_epoch, valid_epoch\n",
    "from  torch.optim import AdamW\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# 'DistilRoBERta-base-cased' 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis'\n",
    "label_set = LabelSet(labels=[\"Neutral\", \"Positive\", \"Negative\"]) \n",
    "model = DistilBertCrfForNer.from_pretrained('mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis', num_labels=len(label_set.ids_to_label.values()))\n",
    "\n",
    "device = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "len_dataset = len(train_dataset)\n",
    "t_total = len(train_dataset)\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "bert_param_optimizer = list(model.distilbert.named_parameters())\n",
    "crf_param_optimizer = list(model.crf.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay, 'lr': config.lr_crf},\n",
    "        {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n",
    "         'lr': config.lr_crf},\n",
    "\n",
    "        {'params': [p for n, p in crf_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay, 'lr': config.crf_learning_rate},\n",
    "        {'params': [p for n, p in crf_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n",
    "         'lr': config.crf_learning_rate},\n",
    "\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=1e-6)\n",
    "warmup_steps = int(t_total * config.warm_up_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "\n",
    "EPOCHS = config.epoch_num # training epoch\n",
    "for e in range(EPOCHS):\n",
    "    print(\"=======START TRAIN EPOCHS %d=======\" %(e+1))\n",
    "    train_loss = train_epoch(e, model, train_loader, optimizer, scheduler,device)\n",
    "    valid_epoch(e, model, val_loader,device,label_set)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
