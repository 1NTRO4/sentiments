{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T05:48:48.537247400Z",
     "start_time": "2023-06-01T05:48:48.436252400Z"
    }
   },
   "outputs": [],
   "source": [
    "## Load The Data\n",
    "import json\n",
    "import os\n",
    "#raw = json.load(open('./data/annotations_consistent.json'))\n",
    "\n",
    "raw = json.load(open('./data/FinEntity.json'))\n",
    "# raw = data.get(\"examples\")\n",
    "\n",
    "# for example in raw:\n",
    "#     for annotation in example['annotations']:\n",
    "#         #We expect the key of label to be label but the data has tag\n",
    "#         annotation['label'] = annotation['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.38.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n",
      "13\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "## Preparing Sequence Labeling Data for Transformers\n",
    "from sequence_aligner.labelset import LabelSet\n",
    "from sequence_aligner.dataset import TrainingDatasetCRF\n",
    "from sequence_aligner.containers import TraingingBatch\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "label_set = LabelSet(labels=[\"Neutral\", \"Positive\", \"Negative\"])  # label in this dataset\n",
    "print(label_set.ids_to_label)\n",
    "print(len(label_set.ids_to_label.values()))\n",
    "dataset = TrainingDatasetCRF(data=raw, tokenizer=tokenizer, label_set=label_set,tokens_per_batch = 128)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 6, 483, 1342, 585, 14522, 17, 15979, 223, 1146, 739, 293, 11, 13420, 10, 557, 988, 8, 2513, 4462, 10374, 9390, 17119, 5108, 17, 67, 129, 24, 1146, 739, 293, 358, 3174, 746, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "on the positive side, siemens is rallying 6 % after a boom in quarterly orders and packaging maker huhtamaki is also up by 6 % after profit beat expectations. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "{0: 'O', 1: 'B-Neutral', 2: 'I-Neutral', 3: 'L-Neutral', 4: 'U-Neutral', 5: 'B-Positive', 6: 'I-Positive', 7: 'L-Positive', 8: 'U-Positive', 9: 'B-Negative', 10: 'I-Negative', 11: 'L-Negative', 12: 'U-Negative'}\n"
     ]
    }
   ],
   "source": [
    "## Prepare train data and valid data\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import config\n",
    "train_size = int(config.dev_split_size * len(dataset))\n",
    "validate_size = len(dataset) - train_size\n",
    "train_dataset, validate_dataset = random_split(dataset, [train_size, validate_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "val_loader = DataLoader(validate_dataset, batch_size=16, collate_fn=TraingingBatch, shuffle=True, )\n",
    "\n",
    "print(dataset[1].input_ids)\n",
    "print(dataset[1].labels)\n",
    "print(dataset[1].attention_masks)\n",
    "print(tokenizer.decode(dataset[1].input_ids))\n",
    "print(dataset.label_set.ids_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seqeval) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin: 100%|██████████| 442M/442M [01:02<00:00, 7.08MB/s] \n",
      "Some weights of BertCrfForNer were not initialized from the model checkpoint at yiyanghkust/finbert-pretrain and are newly initialized: ['classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======START TRAIN EPOCHS 1=======\n",
      "Epoch: 1, train Loss:39.6564\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.33      0.07      0.11       104\n",
      "     Neutral       0.46      0.46      0.46       237\n",
      "    Positive       0.39      0.11      0.17       100\n",
      "\n",
      "   micro avg       0.44      0.29      0.35       441\n",
      "   macro avg       0.40      0.21      0.25       441\n",
      "weighted avg       0.41      0.29      0.31       441\n",
      "\n",
      "Epoch: 1, train Loss:10.6085\n",
      "=======START TRAIN EPOCHS 2=======\n",
      "Epoch: 2, train Loss:7.6225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.35      0.18      0.24       104\n",
      "     Neutral       0.69      0.62      0.65       237\n",
      "    Positive       0.42      0.58      0.49       100\n",
      "\n",
      "   micro avg       0.55      0.51      0.53       441\n",
      "   macro avg       0.48      0.46      0.46       441\n",
      "weighted avg       0.55      0.51      0.52       441\n",
      "\n",
      "Epoch: 2, train Loss:5.0458\n",
      "=======START TRAIN EPOCHS 3=======\n",
      "Epoch: 3, train Loss:3.3643\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.41      0.51      0.45       104\n",
      "     Neutral       0.72      0.74      0.73       237\n",
      "    Positive       0.51      0.36      0.42       100\n",
      "\n",
      "   micro avg       0.60      0.60      0.60       441\n",
      "   macro avg       0.55      0.54      0.54       441\n",
      "weighted avg       0.60      0.60      0.59       441\n",
      "\n",
      "Epoch: 3, train Loss:4.0601\n",
      "=======START TRAIN EPOCHS 4=======\n",
      "Epoch: 4, train Loss:2.0508\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.47      0.28      0.35       104\n",
      "     Neutral       0.76      0.77      0.76       237\n",
      "    Positive       0.46      0.64      0.54       100\n",
      "\n",
      "   micro avg       0.62      0.62      0.62       441\n",
      "   macro avg       0.56      0.56      0.55       441\n",
      "weighted avg       0.62      0.62      0.61       441\n",
      "\n",
      "Epoch: 4, train Loss:4.0597\n",
      "=======START TRAIN EPOCHS 5=======\n",
      "Epoch: 5, train Loss:1.3464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.45      0.34      0.38       104\n",
      "     Neutral       0.78      0.74      0.76       237\n",
      "    Positive       0.42      0.62      0.50       100\n",
      "\n",
      "   micro avg       0.60      0.62      0.61       441\n",
      "   macro avg       0.55      0.56      0.55       441\n",
      "weighted avg       0.62      0.62      0.61       441\n",
      "\n",
      "Epoch: 5, train Loss:4.3506\n",
      "=======START TRAIN EPOCHS 6=======\n",
      "Epoch: 6, train Loss:0.9128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.55      0.61       104\n",
      "     Neutral       0.77      0.76      0.77       237\n",
      "    Positive       0.57      0.75      0.65       100\n",
      "\n",
      "   micro avg       0.70      0.71      0.70       441\n",
      "   macro avg       0.68      0.69      0.67       441\n",
      "weighted avg       0.70      0.71      0.70       441\n",
      "\n",
      "Epoch: 6, train Loss:3.5958\n",
      "=======START TRAIN EPOCHS 7=======\n",
      "Epoch: 7, train Loss:0.4396\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.54      0.65       104\n",
      "     Neutral       0.77      0.83      0.80       237\n",
      "    Positive       0.66      0.81      0.73       100\n",
      "\n",
      "   micro avg       0.75      0.76      0.75       441\n",
      "   macro avg       0.75      0.73      0.73       441\n",
      "weighted avg       0.76      0.76      0.75       441\n",
      "\n",
      "Epoch: 7, train Loss:3.9182\n",
      "=======START TRAIN EPOCHS 8=======\n",
      "Epoch: 8, train Loss:0.2624\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.82      0.74       104\n",
      "     Neutral       0.80      0.71      0.76       237\n",
      "    Positive       0.70      0.81      0.75       100\n",
      "\n",
      "   micro avg       0.74      0.76      0.75       441\n",
      "   macro avg       0.73      0.78      0.75       441\n",
      "weighted avg       0.75      0.76      0.75       441\n",
      "\n",
      "Epoch: 8, train Loss:4.1740\n",
      "=======START TRAIN EPOCHS 9=======\n",
      "Epoch: 9, train Loss:0.1281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.71      0.77       104\n",
      "     Neutral       0.80      0.75      0.77       237\n",
      "    Positive       0.64      0.91      0.75       100\n",
      "\n",
      "   micro avg       0.76      0.78      0.77       441\n",
      "   macro avg       0.76      0.79      0.76       441\n",
      "weighted avg       0.77      0.78      0.77       441\n",
      "\n",
      "Epoch: 9, train Loss:4.0332\n",
      "=======START TRAIN EPOCHS 10=======\n",
      "Epoch: 10, train Loss:0.0704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.78      0.73      0.75       104\n",
      "     Neutral       0.77      0.78      0.78       237\n",
      "    Positive       0.74      0.87      0.80       100\n",
      "\n",
      "   micro avg       0.76      0.79      0.78       441\n",
      "   macro avg       0.76      0.79      0.78       441\n",
      "weighted avg       0.76      0.79      0.78       441\n",
      "\n",
      "Epoch: 10, train Loss:4.7780\n",
      "=======START TRAIN EPOCHS 11=======\n",
      "Epoch: 11, train Loss:0.0399\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.77      0.74       104\n",
      "     Neutral       0.78      0.80      0.79       237\n",
      "    Positive       0.80      0.83      0.81       100\n",
      "\n",
      "   micro avg       0.77      0.80      0.78       441\n",
      "   macro avg       0.77      0.80      0.78       441\n",
      "weighted avg       0.77      0.80      0.78       441\n",
      "\n",
      "Epoch: 11, train Loss:4.7369\n",
      "=======START TRAIN EPOCHS 12=======\n",
      "Epoch: 12, train Loss:0.0251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.78      0.75       104\n",
      "     Neutral       0.79      0.80      0.80       237\n",
      "    Positive       0.84      0.83      0.83       100\n",
      "\n",
      "   micro avg       0.78      0.80      0.79       441\n",
      "   macro avg       0.78      0.80      0.79       441\n",
      "weighted avg       0.79      0.80      0.79       441\n",
      "\n",
      "Epoch: 12, train Loss:4.4078\n",
      "=======START TRAIN EPOCHS 13=======\n",
      "Epoch: 13, train Loss:0.0203\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.74      0.78      0.76       104\n",
      "     Neutral       0.79      0.80      0.79       237\n",
      "    Positive       0.83      0.86      0.85       100\n",
      "\n",
      "   micro avg       0.79      0.81      0.80       441\n",
      "   macro avg       0.79      0.81      0.80       441\n",
      "weighted avg       0.79      0.81      0.80       441\n",
      "\n",
      "Epoch: 13, train Loss:4.9434\n",
      "=======START TRAIN EPOCHS 14=======\n",
      "Epoch: 14, train Loss:0.0219\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.78      0.78       104\n",
      "     Neutral       0.79      0.82      0.80       237\n",
      "    Positive       0.83      0.88      0.85       100\n",
      "\n",
      "   micro avg       0.80      0.82      0.81       441\n",
      "   macro avg       0.80      0.83      0.81       441\n",
      "weighted avg       0.80      0.82      0.81       441\n",
      "\n",
      "Epoch: 14, train Loss:4.9808\n",
      "=======START TRAIN EPOCHS 15=======\n",
      "Epoch: 15, train Loss:0.0138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.78      0.78       104\n",
      "     Neutral       0.78      0.83      0.81       237\n",
      "    Positive       0.85      0.86      0.86       100\n",
      "\n",
      "   micro avg       0.80      0.83      0.81       441\n",
      "   macro avg       0.81      0.82      0.82       441\n",
      "weighted avg       0.80      0.83      0.81       441\n",
      "\n",
      "Epoch: 15, train Loss:5.2076\n",
      "=======START TRAIN EPOCHS 16=======\n",
      "Epoch: 16, train Loss:0.0102\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.78      0.77       104\n",
      "     Neutral       0.79      0.81      0.80       237\n",
      "    Positive       0.82      0.87      0.84       100\n",
      "\n",
      "   micro avg       0.79      0.82      0.80       441\n",
      "   macro avg       0.79      0.82      0.81       441\n",
      "weighted avg       0.79      0.82      0.80       441\n",
      "\n",
      "Epoch: 16, train Loss:5.4435\n",
      "=======START TRAIN EPOCHS 17=======\n",
      "Epoch: 17, train Loss:0.0083\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.78      0.77       104\n",
      "     Neutral       0.79      0.81      0.80       237\n",
      "    Positive       0.82      0.87      0.84       100\n",
      "\n",
      "   micro avg       0.79      0.82      0.80       441\n",
      "   macro avg       0.79      0.82      0.81       441\n",
      "weighted avg       0.79      0.82      0.80       441\n",
      "\n",
      "Epoch: 17, train Loss:4.8503\n",
      "=======START TRAIN EPOCHS 18=======\n",
      "Epoch: 18, train Loss:0.0089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.78      0.77       104\n",
      "     Neutral       0.79      0.81      0.80       237\n",
      "    Positive       0.82      0.87      0.84       100\n",
      "\n",
      "   micro avg       0.79      0.82      0.80       441\n",
      "   macro avg       0.79      0.82      0.81       441\n",
      "weighted avg       0.79      0.82      0.80       441\n",
      "\n",
      "Epoch: 18, train Loss:4.8280\n",
      "=======START TRAIN EPOCHS 19=======\n",
      "Epoch: 19, train Loss:0.0127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.78      0.77       104\n",
      "     Neutral       0.79      0.81      0.80       237\n",
      "    Positive       0.82      0.87      0.84       100\n",
      "\n",
      "   micro avg       0.79      0.82      0.80       441\n",
      "   macro avg       0.79      0.82      0.81       441\n",
      "weighted avg       0.79      0.82      0.80       441\n",
      "\n",
      "Epoch: 19, train Loss:4.7816\n",
      "=======START TRAIN EPOCHS 20=======\n",
      "Epoch: 20, train Loss:0.0123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.76      0.78      0.77       104\n",
      "     Neutral       0.79      0.81      0.80       237\n",
      "    Positive       0.82      0.87      0.84       100\n",
      "\n",
      "   micro avg       0.79      0.82      0.80       441\n",
      "   macro avg       0.79      0.82      0.81       441\n",
      "weighted avg       0.79      0.82      0.80       441\n",
      "\n",
      "Epoch: 20, train Loss:4.8489\n"
     ]
    }
   ],
   "source": [
    "## Bert+Crf\n",
    "import warnings\n",
    "from model.bert_crf import BertCrfForNer\n",
    "from seqeval import metrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from seqeval.metrics import f1_score, precision_score, accuracy_score\n",
    "from torch import cuda\n",
    "import config\n",
    "from util.train import train_epoch, valid_epoch\n",
    "from  torch.optim import AdamW\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# 'bert-base-cased' 'yiyanghkust/finbert-pretrain'\n",
    "label_set = LabelSet(labels=[\"Neutral\", \"Positive\", \"Negative\"]) \n",
    "model = BertCrfForNer.from_pretrained('yiyanghkust/finbert-pretrain', num_labels=len(label_set.ids_to_label.values()))\n",
    "\n",
    "device = 'cuda:0' if cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "len_dataset = len(train_dataset)\n",
    "t_total = len(train_dataset)\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "bert_param_optimizer = list(model.bert.named_parameters())\n",
    "crf_param_optimizer = list(model.crf.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay, 'lr': config.lr_crf},\n",
    "        {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n",
    "         'lr': config.lr_crf},\n",
    "\n",
    "        {'params': [p for n, p in crf_param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': config.weight_decay, 'lr': config.crf_learning_rate},\n",
    "        {'params': [p for n, p in crf_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n",
    "         'lr': config.crf_learning_rate},\n",
    "\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr, eps=1e-6)\n",
    "warmup_steps = int(t_total * config.warm_up_ratio)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps,\n",
    "                                                num_training_steps=t_total)\n",
    "\n",
    "EPOCHS = config.epoch_num # training epoch\n",
    "for e in range(EPOCHS):\n",
    "    print(\"=======START TRAIN EPOCHS %d=======\" %(e+1))\n",
    "    train_loss = train_epoch(e, model, train_loader, optimizer, scheduler,device)\n",
    "    valid_epoch(e, model, val_loader,device,label_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "import pickle\n",
    "\n",
    "# with open('model_bert_crf', 'wb') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
